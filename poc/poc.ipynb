{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062fa0e7-c01d-43dc-916b-396b71b5c661",
   "metadata": {},
   "source": [
    "## Healthcare Data Q&A Assistant with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71adba99-a474-4f61-9722-5c360268dd5f",
   "metadata": {},
   "source": [
    "### Executive Summary\n",
    "\n",
    "This proof of concept demonstrates how structured healthcare data can be transformed into a natural-language Q&A assistant using a retrieval-augmented generation (RAG) pipeline. The system allows users to ask questions such as \"What was the heart disease mortality rate in Texas in 2019?\" and receive answers grounded in real data. The pipeline combines sentences embeddings, vector search with FAISS, a simple query classification layer, and a large language model (LLM) via Ollama. While the prototype is not production-ready, it validates the feasibility of the approach and highlights both strengths (accurate  lookup queries) and limitations (aggregation queries and hallucinations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7effa975-0faa-474b-886d-2090ad74f813",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Healthcare data is often stored in large tabular datasets, making it difficult for non-technical users to extract insights. Analysts typically rely on SQL queries or dashboards, which require technical expertise. Natural language interfaces offer a more intuitive alternative, but raw LLMs are prone to hallucinations and cannot directly \"understand\" structured data.\n",
    "\n",
    "This project explores the use of retrieval-augmented generation (RAG) as a bridge between structured healthcare data and natural-language questions. The central idea is to represent each data row as a natural-language \"fact\", retrieve the most relevant facts for a given query, and let the LLM generate an answer constrained by those facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19bf848d-31df-4a9b-9b32-5f7bd14f0acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/llingcaro/Documents/Work/git_repos/healthcare_data_explorer/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import subprocess\n",
    "import json\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111049c-146a-40aa-9486-ced1b59b5f89",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "\n",
    "The dataset used in this project was obtained from the CDC WONDER database (Centers for Disease Control and Prevention). This project uses the data solely for educational and research purposes.\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The dataset used in this POC contains mortality rates by US state, cause of death, and year. For example, one row might state that the heart disease mortality rate in Texas in 2019 was 150. The dataset was cleaned and formatted into a structured CSV. A quick inspection shows rows with columns for state, year, cause of death, number of deaths, population, and crude mortality rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82dd5a22-3cf5-446c-8467-ccc68a28abc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>year</th>\n",
       "      <th>cause</th>\n",
       "      <th>deaths</th>\n",
       "      <th>population</th>\n",
       "      <th>crude_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Certain other intestinal infections</td>\n",
       "      <td>159.0</td>\n",
       "      <td>4858979.0</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Tuberculosis</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4858979.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Septicemia</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>4858979.0</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Viral hepatitis</td>\n",
       "      <td>96.0</td>\n",
       "      <td>4858979.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Human immunodeficiency virus  disease</td>\n",
       "      <td>126.0</td>\n",
       "      <td>4858979.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state    year                                  cause  deaths  population  \\\n",
       "0  Alabama  2015.0    Certain other intestinal infections   159.0   4858979.0   \n",
       "1  Alabama  2015.0                           Tuberculosis    11.0   4858979.0   \n",
       "2  Alabama  2015.0                             Septicemia  1046.0   4858979.0   \n",
       "3  Alabama  2015.0                        Viral hepatitis    96.0   4858979.0   \n",
       "4  Alabama  2015.0  Human immunodeficiency virus  disease   126.0   4858979.0   \n",
       "\n",
       "   crude_rate  \n",
       "0         3.3  \n",
       "1         0.2  \n",
       "2        21.5  \n",
       "3         2.0  \n",
       "4         2.6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../poc/data/Underlying Cause of Death, 1999-2020.xls\", sep=\"\\t\")\n",
    "df = df.rename(\n",
    "    columns = {\n",
    "        'State':'state',\n",
    "        'Year':'year',\n",
    "        'ICD-10 113 Cause List':'cause',\n",
    "        'Deaths':'deaths',\n",
    "        'Population':'population',\n",
    "        'Crude Rate':'crude_rate'\n",
    "    }\n",
    ")\n",
    "df = df[['state','year','cause','deaths','population','crude_rate']]\n",
    "df['crude_rate'] = pd.to_numeric(df['crude_rate'], errors='coerce')\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "df = df.astype({\n",
    "    \"deaths\" : \"float64\",\n",
    "    \"population\": \"float64\"\n",
    "    }    \n",
    ")\n",
    "df['cause'] = (\n",
    "    df['cause']\n",
    "    .str.replace(r\"\\(.*?\\)\",\"\",regex=True)\n",
    "    .str.replace(r\"#\",\"\",regex=True)\n",
    "    .str.strip()\n",
    ") \n",
    "df['crude_rate'] = np.round(df['deaths'] / df['population'] * 1e5,1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88855f81-c4ab-412b-90fa-5927385bee4e",
   "metadata": {},
   "source": [
    "### Convering Data into Facts\n",
    "\n",
    "To make the dataset usable in a RAG system, each row was converted into a self-contained natural-language statement, referred to as a \"fact\". This approach enables the embedding model and LLM to process the information in text form, which is more natural for semantic search and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee0bf92-07eb-413e-ac90-324b2d414c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In Alabama in 2015.0, the Certain other intestinal infections mortality rate was 3.3 per 100,000.', 'In Alabama in 2015.0, the Tuberculosis mortality rate was 0.2 per 100,000.', 'In Alabama in 2015.0, the Septicemia mortality rate was 21.5 per 100,000.', 'In Alabama in 2015.0, the Viral hepatitis mortality rate was 2.0 per 100,000.', 'In Alabama in 2015.0, the Human immunodeficiency virus  disease mortality rate was 2.6 per 100,000.']\n"
     ]
    }
   ],
   "source": [
    "facts = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    fact = f\"In {row['state']} in {row['year']}, the {row['cause']} mortality rate was {row['crude_rate']} per 100,000.\"\n",
    "    facts.append(fact)\n",
    "\n",
    "print(facts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c01d0-3a36-43a2-8f76-28342a9963c5",
   "metadata": {},
   "source": [
    "### Embeddings and Vector Index\n",
    "\n",
    "Both queries and facts are mapped into a shared semantic vector space using sentence embeddings. This allows the system to retrieve the most relevant facts for a given query based on similarity rather than exact keyword matching.\n",
    "\n",
    "A FAISS index was built to enable fast near-neighbor search across all fact embeddings. This index is central to the retrieval process in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "651ed06b-9a26-4c75-9751-0e00b510ac64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18805, 384)\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "fact_embeddings = embedder.encode(facts, convert_to_numpy=True)\n",
    "print(fact_embeddings.shape)\n",
    "\n",
    "dimension = fact_embeddings.shape[1]\n",
    "# set up a search engine that compares vectors by distance\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "# load all the fact embeddings into the search engine so they can be queried later\n",
    "index.add(fact_embeddings.astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc8238f-2a63-4959-a5d7-4f27bf6bfdd8",
   "metadata": {},
   "source": [
    "### Query Classification Layer\n",
    "\n",
    "Not all queries are equal. Some are simple lookups (e.g., \"What was the heart disease rate in Texas in 2019?\"), while others are aggregations (e.g., \"What are the top 3 states with the highest heart disease mortality in 2019?\").\n",
    "\n",
    "A rules-based query classification function was implemented to distinguish between these categories. Lookup queries are answered using a small number of retrieved facts to maximize precision. Aggregation queries require a larger set of retrieved facts to provide broader context for the LLM. \n",
    "\n",
    "### Retrieval-Augmented Generation (RAG) Pipeline\n",
    "\n",
    "The complete pipeline works as follows:\n",
    "\n",
    "1. The user query is classified as either lookup or aggregation.\n",
    "2. The query is embedded and searched against the FAISS index.\n",
    "3. The top-k most relevant facts are retrieved.\n",
    "4. The retrieved facts are passed as context to an LLM running via Ollama.\n",
    "5. The LLM generates an answer, constrained by the retrieved facts.\n",
    "\n",
    "This ensures that the model's answers are grounded in actual data whenever possible. See some examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8dc0751-0c8c-4d83-a5bb-131f7f41ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_k(query):\n",
    "    query = query.lower()\n",
    "    if any(word in query for word in [\n",
    "                \"top\", \"highest\", \"lowest\", \n",
    "                \"average\", \"most\", \"least\",\n",
    "                \"trend\",\"rank\"]):\n",
    "        return 20\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def rag_pipeline(query):\n",
    "    \n",
    "    k = classify_k(query)\n",
    "\n",
    "    query_embeddings = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(query_embeddings.astype(\"float32\"), k=k)\n",
    "    retrieved_facts = [facts[i] for i in I[0]]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. Use only the facts provided to answer the question. \n",
    "    If the answer is not in the facts, say \"I don't have data on that.\"\n",
    "\n",
    "    Facts:\n",
    "    {retrieved_facts}\n",
    "\n",
    "    Question:\n",
    "\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    process = subprocess.run(\n",
    "        [\"ollama\", \"run\", \"mistral\", prompt],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    print(process.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed400372-635a-46ec-ad2f-3ddf5cf510c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The total heart disease mortality rate in Texas in 2019 was the sum of the Diseases of heart mortality rate and Other heart diseases mortality rate. So, it would be 159.1 (Diseases of heart mortality rate) + 52.0 (Other heart diseases mortality rate) = 211.1 per 100,000.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline(\"What was the heart disease mortality rate in Texas in 2019?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "881ced92-8eca-473f-8ca9-24a2912a272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't have data on that. The provided facts only show the rates for the years 2016 and 2017, neither of which is 85 per 100,000.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline(\"In which year did Texas have a diabetes mortality rate of 85?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0d43c6c-19ae-48d2-8a94-f7c70f280233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't have data on that. The provided facts only mention Influenza and pneumonia mortality rates for Texas in 2018 and 2019, not COVID-19.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline(\"What was the COVID-19 mortality rate in Texas in 2018?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95972f58-b8ca-4f97-ab2e-45b1f4a9e089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To rank the top 10 states by Other heart diseases mortality rate in ascending order for the year 2019, we can arrange the data as follows:\n",
      "\n",
      "1. Alaska: 42.3 per 100,000 (in Alaska, 2018.0)\n",
      "2. Washington: 44.8 per 100,000 (in Washington, 2019.0)\n",
      "3. California: 46.4 per 100,000 (in California, 2018.0)\n",
      "4. Mississippi: 97.6 per 100,000 (in Mississippi, 2019.0)\n",
      "5. Massachusetts: 76.2 per 100,000 (in Massachusetts, 2019.0)\n",
      "6. Maryland: 63.6 per 100,000 (in Maryland, 2019.0)\n",
      "7. Pennsylvania: Not provided for the year 2019 in the given data set. However, it was 103.0 per 100,000 (in Pennsylvania, 2019.0), and 101.2 per 100,000 (in Pennsylvania, 2020.0).\n",
      "8. Alabama: Not provided in the given data set. However, we can infer that it must have a higher rate than Pennsylvania's 2019 rate of 103.0 per 100,000 since other states with lower rates (like Massachusetts and Maryland) are ranked above Pennsylvania.\n",
      "9. Not enough data to determine the remaining rankings for states with missing 2019 data or higher rates than Alabama (if it exists).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline(\"Rank the top 10 states by heart disease mortality in 2019 in ascending order.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd9df375-54df-4ccd-a372-36c42200df94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Septicemia mortality rate in Arizona has a downward trend from 2015 to 2020, as the rate decreased from 6.5 per 100,000 in 2015 to 4.7 per 100,000 in 2019, and then slightly increased to 5.7 per 100,000 in 2020.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline(\"What is the trend for Septicemia mortality rate in Arizona from 2015 to 2020?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb63d9-63dd-41a0-babd-277ffd05c209",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The system performed well on lookup queries. When only a few facts were retrieved, answers were accurate and concise, demonstrating that this approach works reliably for direct fact-based questions. Aggregation queries performed less consistently. A larger k improved results by giving the LLM more context, but the model sometimes hallucinated rankings or summaries rather than calculating them. For out-of-scope queries such as COVID mortality, the model responds as instructed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31250e7-3a33-4684-940c-407285ec316b",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This POC demonstrates the feasibility of building a healthcare data Q&A assistant using RAG. The project shows that structured tabular data can be transformed into natural-language facts, that embedding and retrieval provide a scalable foundation for semantic search, and that even a lightweight query classification mechanism improves adaptabiliy. Lookup queries performed strongly, while aggregation queries highlighted the limitations of relying on the LLM without structured computation. Although this system is not production-ready, it validates the overall approach and serves as a strong foundation for future development. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37397e94-d780-4257-9bb2-fe20d50d63ab",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "The next logical improvements include replacing the rules-based query classifier with an LLM-based approach for greater flexibility, adding structured aggregation logic with tools such as Pandas or SQL to ensure accuracy in summary queries.\n",
    "\n",
    "Another important enhancement is the addition of validation mechanisms. The system should not only provide an answer, but also present the supporting facts, similarity scores, and links back to the original dataset rows so that users can verify correctness. This transparency is especially critical in the healthcare domain. Further steps include expanding the dataset to test performance at scale with formal evaluation metrics, and wrapping the pipeline in a simple application interface using Streamlit or FastAPI to turn the notebook into a working demo.\n",
    "\n",
    "In summary , this POC achieved its goal of proving that healthcare data can be queried in natural language using a RAG pipeline. It demonstrates clear strengths, identifies areas for growth, and establishes a solid foundation for moving from prototype to a more advanced application. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
